{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Def2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Environment preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed, parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincenzo/anaconda3/envs/vstk/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from vstk.models import Def2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vstk.tokenisers import NLTKTokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vstk.data import CoNLL2003, SST, STS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODEL_DIR_PATH = '../resources/models/pre_trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CONLL2003_DATA_PATH = '../resources/data/raw/conll2003'\n",
    "SST_DATA_PATH = '../resources/data/raw/stanfordSentimentTreebank'\n",
    "STS_DATA_PATH = '../resources/data/raw/stsbenchmark'\n",
    "WIKTIONARY_DATA_PATH = '../resources/data/raw/wiktionary/en/word_def_dictionary.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('../resources/'):\n",
    "    os.mkdir('../resources/')\n",
    "if not os.path.exists('../resources/models'):\n",
    "    os.mkdir('../resources/models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dictionary</td>\n",
       "      <td>( by extension ) any work that has a list of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>free</td>\n",
       "      <td>not imprisoned or enslaved .\\n\\nunconstrained ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thesaurus</td>\n",
       "      <td>( archaic ) a dictionary or encyclopedia .\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>encyclopedia</td>\n",
       "      <td>( dated ) the circle of arts and sciences ; a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>portmanteau</td>\n",
       "      <td>a large travelling case usually made of leathe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>encyclopaedia</td>\n",
       "      <td>( chiefly , uk , australia ) alternative spell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cat</td>\n",
       "      <td>an animal of the family &lt;unk&gt; :\\n\\na person :\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gratis</td>\n",
       "      <td>free , without charge .\\n\\nfree , without char...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>word</td>\n",
       "      <td>the smallest discrete unit of spoken language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>livre</td>\n",
       "      <td>( historical ) a unit of currency formerly use...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title                                               text\n",
       "0     dictionary  ( by extension ) any work that has a list of m...\n",
       "1           free  not imprisoned or enslaved .\\n\\nunconstrained ...\n",
       "2      thesaurus  ( archaic ) a dictionary or encyclopedia .\\n\\n...\n",
       "3   encyclopedia  ( dated ) the circle of arts and sciences ; a ...\n",
       "4    portmanteau  a large travelling case usually made of leathe...\n",
       "5  encyclopaedia  ( chiefly , uk , australia ) alternative spell...\n",
       "6            cat  an animal of the family <unk> :\\n\\na person :\\...\n",
       "7         gratis  free , without charge .\\n\\nfree , without char...\n",
       "8           word  the smallest discrete unit of spoken language ...\n",
       "9          livre  ( historical ) a unit of currency formerly use..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(WIKTIONARY_DATA_PATH, dtype=str)\n",
    "# TODO find better solution\n",
    "tmp_df = df[df.isna().any(axis=1)]\n",
    "df = pd.concat([df[~df.isna().any(axis=1)], pd.DataFrame.from_dict([{'title': 'nan', 'text': '\\n\\n'.join(tmp_df['text'])}])], axis=0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokeniser = NLTKTokeniser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conll2003 = {split: CoNLL2003(CONLL2003_DATA_PATH, split=split) for split in ['train', 'validation', 'test']}\n",
    "sst = {split: SST(SST_DATA_PATH, split=split) for split in ['train', 'validation', 'test']}\n",
    "sts = {split: STS(STS_DATA_PATH, split=split) for split in ['train', 'validation', 'test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_configs_hash(**configs):\n",
    "    return hashlib.sha256(str(configs).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Def2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data set analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in Wiktionary data: 10569365\n",
      "Tokens in CoNLL data: 301418\n",
      "Tokens in Stanford Sentiment Treebank data: 220353\n",
      "Tokens in Semantic Textual Similarity data: 197467\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading', n_jobs=-1):  # , verbose=2):\n",
    "    wiktionary_data = [\n",
    "        token\n",
    "        for sample in Parallel()(\n",
    "            delayed(lambda x: tokeniser(f'{x['title']}\\n\\n{x['text']}'))(row) \n",
    "            for _, row in df.iterrows()\n",
    "        )\n",
    "        for token in sample\n",
    "    ]\n",
    "    print(f'Tokens in Wiktionary data: {len(wiktionary_data)}')\n",
    "    conll2003_data = [elem['token'] for split in conll2003 for sample in conll2003[split] for elem in sample]\n",
    "    print(f'Tokens in CoNLL data: {len(conll2003_data)}')\n",
    "    sst_data = [\n",
    "        token\n",
    "        for sample in Parallel()(\n",
    "            delayed(lambda x: tokeniser(x['sequence']))(elem) \n",
    "            for split in sst for elem in sst[split]\n",
    "        )\n",
    "        for token in sample\n",
    "    ]\n",
    "    print(f'Tokens in Stanford Sentiment Treebank data: {len(sst_data)}')\n",
    "    sts_data = [\n",
    "        token\n",
    "        for sample in Parallel()(\n",
    "            delayed(lambda x, y: tokeniser(x[f'sequence_{y}']))(elem, i) \n",
    "            for split in sts for elem in sts[split] for i in 'ab'\n",
    "        )\n",
    "        for token in sample\n",
    "    ]\n",
    "    print(f'Tokens in Semantic Textual Similarity data: {len(sts_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(token.lower() for token in wiktionary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequent_words_wiktionary = set(token.lower() for token, count in counts.items() if count > 10)\n",
    "infrequent_words_conll2003 = set(conll2003_data) - frequent_words_wiktionary\n",
    "infrequent_words_sst = set(sst_data) - frequent_words_wiktionary\n",
    "infrequent_words_sts = set(sts_data) - frequent_words_wiktionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_to_restore = infrequent_words_conll2003 | infrequent_words_sst | infrequent_words_sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiktionary_data_reduced = df[df['title'].apply(lambda x: x.lower()).isin(frequent_words_wiktionary)]\n",
    "wiktionary_data_to_restore = df[df['title'].apply(lambda x: x.lower()).isin(words_to_restore)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiktionary entries: 764593\n",
      "Wiktionary tokens: 10569365\n",
      "Frequent Wiktionary tokens: 30173\n"
     ]
    }
   ],
   "source": [
    "print(f'Wiktionary entries: {len(df)}')\n",
    "print(f'Wiktionary tokens: {len(wiktionary_data)}')\n",
    "print(f'Frequent Wiktionary tokens: {len(wiktionary_data_reduced)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conll2003\n",
      "\t train 14041\n",
      "\t validation 3250\n",
      "\t test 3453\n",
      "sst\n",
      "\t train 8117\n",
      "\t validation 2125\n",
      "\t test 1044\n",
      "sts\n",
      "\t train 5749\n",
      "\t validation 1500\n",
      "\t test 1379\n"
     ]
    }
   ],
   "source": [
    "for name, data in zip(['conll2003', 'sst', 'sts'], [conll2003, sst, sts]):\n",
    "    print(name)\n",
    "    for split, samples in data.items():\n",
    "        print('\\t', split, len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conll2003\n",
      "\t train 14.501887329962253\n",
      "\t validation 15.803692307692307\n",
      "\t test 13.447726614538082\n",
      "sst\n",
      "\t train 19.614759147468277\n",
      "\t validation 19.248\n",
      "\t test 19.385057471264368\n",
      "sts\n",
      "\t train 11.130979300747956\n",
      "\t validation 12.940333333333333\n",
      "\t test 11.117476432197245\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-1):  # , verbose=2):\n",
    "    print('conll2003')\n",
    "    for split, data in conll2003.items():\n",
    "        print('\\t', split, np.mean([len(sample) for sample in data]))\n",
    "    print('sst')\n",
    "    for split, data in sst.items():\n",
    "        print('\\t', split, np.mean(\n",
    "            Parallel()(\n",
    "                delayed(lambda x: len(tokeniser(x['sequence'])))(sample) \n",
    "                for sample in data\n",
    "            )))\n",
    "    print('sts')\n",
    "    for split, data in sts.items():\n",
    "        print('\\t', split, np.mean(\n",
    "            Parallel()(\n",
    "                delayed(lambda x, y: len(tokeniser(x[f'sequence_{y}'])))(sample, key) \n",
    "                for sample in data for key in 'ab'\n",
    "            )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conll2003\n",
      "\t train 21009\n",
      "\t validation 9002\n",
      "\t test 8548\n",
      "sst\n",
      "\t train 16310\n",
      "\t validation 7715\n",
      "\t test 4856\n",
      "sts\n",
      "\t train 12467\n",
      "\t validation 6567\n",
      "\t test 4882\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading', n_jobs=-1):  # , verbose=2):\n",
    "    print('conll2003')\n",
    "    for split, data in conll2003.items():\n",
    "        print('\\t', split, len(set([elem['token'].lower() for sample in data for elem in sample])))\n",
    "    print('sst')\n",
    "    for split, data in sst.items():\n",
    "        print('\\t', split, len(set(\n",
    "            tok.lower() for s in Parallel()(\n",
    "                delayed(lambda x: tokeniser(x['sequence']))(sample) \n",
    "                for sample in data\n",
    "            ) for tok in s\n",
    "        )))\n",
    "    print('sts')\n",
    "    for split, data in sts.items():\n",
    "        print('\\t', split, len(set(\n",
    "            tok.lower() for s in Parallel()(\n",
    "                delayed(lambda x, y: tokeniser(x[f'sequence_{y}']))(sample, key) \n",
    "                for sample in data for key in 'ab'\n",
    "            ) for tok in s\n",
    "        )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conll2003\n",
      "\t train 0.5929777081404458\n",
      "\t validation 0.5941538461538461\n",
      "\t test 0.5676223573704026\n",
      "sst\n",
      "\t train 0.6862141185166933\n",
      "\t validation 0.6804705882352942\n",
      "\t test 0.6973180076628352\n",
      "sts\n",
      "\t train 0.5289615585319186\n",
      "\t validation 0.48133333333333334\n",
      "\t test 0.37055837563451777\n"
     ]
    }
   ],
   "source": [
    "infrequent_words_wiktionary = set(token.lower() for token in counts) - frequent_words_wiktionary\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-1):  # , verbose=2):\n",
    "    print('conll2003')\n",
    "    for split, data in conll2003.items():\n",
    "        tmp = sum(any(elem['token'].lower() in infrequent_words_wiktionary for elem in sample) for sample in data)\n",
    "        print('\\t', split, tmp / len(data))\n",
    "    print('sst')\n",
    "    for split, data in sst.items():\n",
    "        tmp = sum(\n",
    "            Parallel()(\n",
    "                delayed(lambda x: any(elem.lower() in infrequent_words_wiktionary for elem in tokeniser(x['sequence'])))(sample) \n",
    "                for sample in data\n",
    "            )\n",
    "        )\n",
    "        print('\\t', split, tmp / len(data))\n",
    "    print('sts')\n",
    "    for split, data in sts.items():\n",
    "        tmp = sum(\n",
    "            Parallel()(\n",
    "                delayed(lambda x: any(elem.lower() in infrequent_words_wiktionary for key in 'ab' for elem in tokeniser(x[f'sequence_{key}'])))(sample) \n",
    "                for sample in data\n",
    "            )\n",
    "        )\n",
    "        print('\\t', split, tmp / len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conll2003\n",
      "\t train 0.5945071159979056\n",
      "\t validation 0.475672072872695\n",
      "\t test 0.49637342068320073\n",
      "sst\n",
      "\t train 0.4270999386879215\n",
      "\t validation 0.3301360985093973\n",
      "\t test 0.2911861614497529\n",
      "sts\n",
      "\t train 0.4058715007620117\n",
      "\t validation 0.25749961930866455\n",
      "\t test 0.2628021302744777\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading', n_jobs=-1):  # , verbose=2):\n",
    "    print('conll2003')\n",
    "    for split, data in conll2003.items():\n",
    "        tmp = set([elem['token'].lower() for sample in data for elem in sample])\n",
    "        print('\\t', split, len(tmp - frequent_words_wiktionary) / len(tmp))\n",
    "    print('sst')\n",
    "    for split, data in sst.items():\n",
    "        tmp = set(\n",
    "            tok.lower() for s in Parallel()(\n",
    "                delayed(lambda x: tokeniser(x['sequence']))(sample) \n",
    "                for sample in data\n",
    "            ) for tok in s\n",
    "        )\n",
    "        print('\\t', split, len(tmp - frequent_words_wiktionary) / len(tmp))\n",
    "    print('sts')\n",
    "    for split, data in sts.items():\n",
    "        tmp = set(\n",
    "            tok.lower() for s in Parallel()(\n",
    "                delayed(lambda x, y: tokeniser(x[f'sequence_{y}']))(sample, key) \n",
    "                for sample in data for key in 'ab'\n",
    "            ) for tok in s\n",
    "        )\n",
    "        print('\\t', split, len(tmp - frequent_words_wiktionary) / len(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Fitting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def2vec_configs = {'subwords': [True, False], 'stopwords': [False, True]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincenzo/anaconda3/envs/vstk/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for i, configs in enumerate([dict(zip(def2vec_configs.keys(), values)) for  values in  product(*def2vec_configs.values())]):\n",
    "    explained_variance.append(list())\n",
    "    for training_data, extend in zip([df, wiktionary_data_reduced], [False, True]):\n",
    "        if extend:\n",
    "            if not os.path.exists(os.path.join(MODEL_DIR_PATH, f'def_2_vec_{get_configs_hash(**configs)}_reduced')):\n",
    "                d2v = Def2Vec(**configs, tokeniser=NLTKTokeniser(bos_token='<s>', eos_token='</s>', unk_token='<unk>'), embedding_size=300)\n",
    "                d2v.fit(training_data.values.tolist())\n",
    "                d2v.build_index()\n",
    "                d2v.save(os.path.join(MODEL_DIR_PATH, f'def_2_vec_{get_configs_hash(**configs)}_reduced'))\n",
    "                with open(os.path.join(MODEL_DIR_PATH, f'def_2_vec_{get_configs_hash(**configs)}_reduced', 'experiment_configs.yml'), 'w') as f:\n",
    "                    yaml.dump({**configs}, f)\n",
    "                explained_variance[i].append(d2v.lsa.explained_variance_ratio_)\n",
    "            else:  # NOTE not very efficient\n",
    "                d2v = Def2Vec.load(path=os.path.join(MODEL_DIR_PATH, f'def_2_vec_{get_configs_hash(**configs)}_reduced'))\n",
    "            if not os.path.exists(os.path.join(MODEL_DIR_PATH, f'def_2_vec_{get_configs_hash(**configs)}_extended')):\n",
    "                d2v.extend_vocabulary(wiktionary_data_to_restore.values.tolist())\n",
    "                d2v.build_index()\n",
    "                d2v.save(os.path.join(MODEL_DIR_PATH, f'def_2_vec_{get_configs_hash(**configs)}_extended'))\n",
    "                with open(os.path.join(MODEL_DIR_PATH, f'def_2_vec_{get_configs_hash(**configs)}_extended', 'experiment_configs.yml'), 'w') as f:\n",
    "                    yaml.dump({**configs, 'extended': True}, f)\n",
    "                explained_variance[i].append(d2v.lsa.explained_variance_ratio_)\n",
    "        else:\n",
    "            if not os.path.exists(os.path.join(MODEL_DIR_PATH, f'def_2_vec_{get_configs_hash(**configs)}')):\n",
    "                d2v = Def2Vec(**configs, tokeniser=NLTKTokeniser(bos_token='<s>', eos_token='</s>', unk_token='<unk>'), embedding_size=300)\n",
    "                d2v.fit(training_data.values.tolist())\n",
    "                d2v.build_index()\n",
    "                d2v.save(os.path.join(MODEL_DIR_PATH, f'def_2_vec_{get_configs_hash(**configs)}'))\n",
    "                with open(os.path.join(MODEL_DIR_PATH, f'def_2_vec_{get_configs_hash(**configs)}', 'experiment_configs.yml'), 'w') as f:\n",
    "                    yaml.dump({**configs}, f)\n",
    "                explained_variance[i].append(d2v.lsa.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 3, sharex=True, sharey=True)\n",
    "\n",
    "for i, configs in tqdm(enumerate([dict(zip(def2vec_configs.keys(), values)) for  values in  product(*def2vec_configs.values())])):\n",
    "    for j, suffix in tqdm(enumerate(['', '_reduced', '_extended'])):\n",
    "        values = explained_variance[i][j]\n",
    "        axes[i, j].plot(np.arange(len(values)), 1. - values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
